{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from google.cloud import bigquery, storage\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = os.getenv(\"BASE_URL\")\n",
    "MONTHS = [f\"{i:02d}\" for i in range(1, 13)] \n",
    "DOWNLOAD_DIR = os.getenv(\"DOWNLOAD_DIR\")\n",
    "CHUNK_SIZE = 8 * 1024 * 1024\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "DATASET_ID = os.getenv(\"DATASET_ID\")\n",
    "TABLE_ID = os.getenv(\"TABLE_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you authenticated through the GCP SDK you can comment out these two lines\n",
    "GOOGLE_APPLICATION_CREDENTIALS=\"D:/DEZommcamp2025/gcp-serviceaccount/gcp-warehouse.json\" \n",
    "client = storage.Client.from_service_account_json(GOOGLE_APPLICATION_CREDENTIALS)\n",
    "# Initialize BigQuery and Storage clients\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "bigquery_client =  bigquery.Client.from_service_account_json(GOOGLE_APPLICATION_CREDENTIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_unzip_file(month):\n",
    "    url = f\"{BASE_URL}fhv_tripdata_2019-{month}.csv.gz\"\n",
    "    gz_file_path = os.path.join(DOWNLOAD_DIR, f\"fhv_tripdata_2019-{month}.csv.gz\")\n",
    "    csv_file_path = os.path.join(DOWNLOAD_DIR, f\"fhv_tripdata_2019-{month}.csv\")\n",
    "\n",
    "    try:\n",
    "        print(f\"Downloading {url}...\")\n",
    "        urllib.request.urlretrieve(url, gz_file_path)\n",
    "        print(f\"Downloaded: {gz_file_path}\")\n",
    "        \n",
    "        # Unzip the file\n",
    "        with gzip.open(gz_file_path, 'rb') as f_in:\n",
    "            with open(csv_file_path, 'wb') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "        print(f\"Unzipped: {csv_file_path}\")\n",
    "        \n",
    "        return csv_file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download or unzip {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-01.csv.gz...\n",
      "Downloading https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-02.csv.gz...\n",
      "Downloading https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-03.csv.gz...\n",
      "Downloading https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-04.csv.gz...\n",
      "Downloaded: ./data\\fhv_tripdata_2019-04.csv.gz\n",
      "Downloaded: ./data\\fhv_tripdata_2019-02.csv.gz\n",
      "Unzipped: ./data\\fhv_tripdata_2019-04.csv\n",
      "Downloading https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-05.csv.gz...\n",
      "Downloaded: ./data\\fhv_tripdata_2019-03.csv.gz\n",
      "Unzipped: ./data\\fhv_tripdata_2019-02.csv\n",
      "Downloading https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-06.csv.gz...\n",
      "Unzipped: ./data\\fhv_tripdata_2019-03.csv\n",
      "Downloading https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-07.csv.gz...\n",
      "Downloaded: ./data\\fhv_tripdata_2019-05.csv.gz\n",
      "Unzipped: ./data\\fhv_tripdata_2019-05.csv\n",
      "Downloading https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-08.csv.gz...\n",
      "Downloaded: ./data\\fhv_tripdata_2019-07.csv.gz\n",
      "Unzipped: ./data\\fhv_tripdata_2019-07.csv\n",
      "Downloading https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-09.csv.gz...\n",
      "Downloaded: ./data\\fhv_tripdata_2019-08.csv.gz\n",
      "Unzipped: ./data\\fhv_tripdata_2019-08.csv\n",
      "Downloading https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz...\n",
      "Downloaded: ./data\\fhv_tripdata_2019-09.csv.gz\n",
      "Unzipped: ./data\\fhv_tripdata_2019-09.csv\n",
      "Downloading https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-11.csv.gz...\n",
      "Downloaded: ./data\\fhv_tripdata_2019-06.csv.gz\n",
      "Unzipped: ./data\\fhv_tripdata_2019-06.csv\n",
      "Downloading https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-12.csv.gz...\n",
      "Downloaded: ./data\\fhv_tripdata_2019-10.csv.gz\n",
      "Unzipped: ./data\\fhv_tripdata_2019-10.csv\n",
      "Downloaded: ./data\\fhv_tripdata_2019-11.csv.gz\n",
      "Downloaded: ./data\\fhv_tripdata_2019-12.csv.gz\n",
      "Unzipped: ./data\\fhv_tripdata_2019-11.csv\n",
      "Unzipped: ./data\\fhv_tripdata_2019-12.csv\n",
      "Downloaded: ./data\\fhv_tripdata_2019-01.csv.gz\n",
      "Unzipped: ./data\\fhv_tripdata_2019-01.csv\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        file_paths = list(executor.map(download_and_unzip_file, MONTHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\n",
    "    bigquery.SchemaField(\"dispatching_base_num\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"dropOff_datetime\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"PUlocationID\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"DOlocationID\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"SR_Flag\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"Affiliated_base_number\", \"STRING\"),\n",
    "   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bq_table_with_schema(dataset_id, table_id, schema):\n",
    "    \n",
    "    # Create the table with the specified schema\n",
    "    table_ref = bigquery_client.dataset(dataset_id).table(table_id)\n",
    "\n",
    "    # Create the table with the specified schema\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = bigquery_client.create_table(table)\n",
    "\n",
    "    print(f\"Created table in BQ {table.project}.{table.dataset_id}.{table.table_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ref = bigquery_client.dataset(dataset_id=DATASET_ID).table(table_id=TABLE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table in BQ de-zoomcamp2025-448100.trips_data_all.fhv_tripdata\n"
     ]
    }
   ],
   "source": [
    "create_bq_table_with_schema(dataset_id=DATASET_ID, table_id=TABLE_ID, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data\\\\fhv_tripdata_2019-01.csv',\n",
       " './data\\\\fhv_tripdata_2019-02.csv',\n",
       " './data\\\\fhv_tripdata_2019-03.csv',\n",
       " './data\\\\fhv_tripdata_2019-04.csv',\n",
       " './data\\\\fhv_tripdata_2019-05.csv',\n",
       " './data\\\\fhv_tripdata_2019-06.csv',\n",
       " './data\\\\fhv_tripdata_2019-07.csv',\n",
       " './data\\\\fhv_tripdata_2019-08.csv',\n",
       " './data\\\\fhv_tripdata_2019-09.csv',\n",
       " './data\\\\fhv_tripdata_2019-10.csv',\n",
       " './data\\\\fhv_tripdata_2019-11.csv',\n",
       " './data\\\\fhv_tripdata_2019-12.csv']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data\\\\fhv_tripdata_2019-09.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00009</td>\n",
       "      <td>2019-09-01 00:35:00</td>\n",
       "      <td>2019-09-01 00:59:00</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00009</td>\n",
       "      <td>2019-09-01 00:48:00</td>\n",
       "      <td>2019-09-01 01:09:00</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-09-01 00:16:18</td>\n",
       "      <td>2019-09-02 00:35:37</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-09-01 00:55:03</td>\n",
       "      <td>2019-09-01 01:09:35</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-09-01 00:13:08</td>\n",
       "      <td>2019-09-02 01:12:31</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num      pickup_datetime     dropOff_datetime  \\\n",
       "0               B00009  2019-09-01 00:35:00  2019-09-01 00:59:00   \n",
       "1               B00009  2019-09-01 00:48:00  2019-09-01 01:09:00   \n",
       "2               B00014  2019-09-01 00:16:18  2019-09-02 00:35:37   \n",
       "3               B00014  2019-09-01 00:55:03  2019-09-01 01:09:35   \n",
       "4               B00014  2019-09-01 00:13:08  2019-09-02 01:12:31   \n",
       "\n",
       "   PUlocationID  DOlocationID  SR_Flag Affiliated_base_number  \n",
       "0         264.0         264.0      NaN                 B00009  \n",
       "1         264.0         264.0      NaN                 B00009  \n",
       "2         264.0         264.0      NaN                 B00014  \n",
       "3         264.0         264.0      NaN                 B00014  \n",
       "4         264.0         264.0      NaN                 B00014  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_bigquery(file_paths):\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            # Load CSV data into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Convert datetime columns to the correct data type\n",
    "            df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\n",
    "            df[\"dropOff_datetime\"] = pd.to_datetime(df[\"dropOff_datetime\"])\n",
    "            \n",
    "            table_ref = bigquery_client.dataset(DATASET_ID).table(TABLE_ID)\n",
    "            \n",
    "            # Load DataFrame to BigQuery\n",
    "            job_config = bigquery.LoadJobConfig(schema=schema)\n",
    "            job = bigquery_client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "            job.result()  # Wait for the load job to complete\n",
    "            \n",
    "            print(f'Loaded {job.output_rows} rows from {file_path} into {DATASET_ID}.{TABLE_ID}.')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload data from {file_path} to BigQuery: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 23143222 rows from ./data\\fhv_tripdata_2019-01.csv into trips_data_all.fhv_tripdata.\n",
      "Loaded 1707649 rows from ./data\\fhv_tripdata_2019-02.csv into trips_data_all.fhv_tripdata.\n",
      "Loaded 1475564 rows from ./data\\fhv_tripdata_2019-03.csv into trips_data_all.fhv_tripdata.\n",
      "Loaded 1937844 rows from ./data\\fhv_tripdata_2019-04.csv into trips_data_all.fhv_tripdata.\n",
      "Loaded 2073045 rows from ./data\\fhv_tripdata_2019-05.csv into trips_data_all.fhv_tripdata.\n",
      "Loaded 2009886 rows from ./data\\fhv_tripdata_2019-06.csv into trips_data_all.fhv_tripdata.\n",
      "Loaded 1947739 rows from ./data\\fhv_tripdata_2019-07.csv into trips_data_all.fhv_tripdata.\n",
      "Loaded 1880407 rows from ./data\\fhv_tripdata_2019-08.csv into trips_data_all.fhv_tripdata.\n",
      "Loaded 1248514 rows from ./data\\fhv_tripdata_2019-09.csv into trips_data_all.fhv_tripdata.\n",
      "Loaded 1897493 rows from ./data\\fhv_tripdata_2019-10.csv into trips_data_all.fhv_tripdata.\n",
      "Loaded 1879137 rows from ./data\\fhv_tripdata_2019-11.csv into trips_data_all.fhv_tripdata.\n",
      "Loaded 2044196 rows from ./data\\fhv_tripdata_2019-12.csv into trips_data_all.fhv_tripdata.\n"
     ]
    }
   ],
   "source": [
    "upload_to_bigquery(filter(None, file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purge_files(directory):\n",
    "    try:\n",
    "        for file_name in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to purge files: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted file: ./data\\fhv_tripdata_2019-01.csv\n",
      "Deleted file: ./data\\fhv_tripdata_2019-01.csv.gz\n",
      "Deleted file: ./data\\fhv_tripdata_2019-02.csv\n",
      "Deleted file: ./data\\fhv_tripdata_2019-02.csv.gz\n",
      "Deleted file: ./data\\fhv_tripdata_2019-03.csv\n",
      "Deleted file: ./data\\fhv_tripdata_2019-03.csv.gz\n",
      "Deleted file: ./data\\fhv_tripdata_2019-04.csv\n",
      "Deleted file: ./data\\fhv_tripdata_2019-04.csv.gz\n",
      "Deleted file: ./data\\fhv_tripdata_2019-05.csv\n",
      "Deleted file: ./data\\fhv_tripdata_2019-05.csv.gz\n",
      "Deleted file: ./data\\fhv_tripdata_2019-06.csv\n",
      "Deleted file: ./data\\fhv_tripdata_2019-06.csv.gz\n",
      "Deleted file: ./data\\fhv_tripdata_2019-07.csv\n",
      "Deleted file: ./data\\fhv_tripdata_2019-07.csv.gz\n",
      "Deleted file: ./data\\fhv_tripdata_2019-08.csv\n",
      "Deleted file: ./data\\fhv_tripdata_2019-08.csv.gz\n",
      "Deleted file: ./data\\fhv_tripdata_2019-09.csv\n",
      "Deleted file: ./data\\fhv_tripdata_2019-09.csv.gz\n",
      "Deleted file: ./data\\fhv_tripdata_2019-10.csv\n",
      "Deleted file: ./data\\fhv_tripdata_2019-10.csv.gz\n",
      "Deleted file: ./data\\fhv_tripdata_2019-11.csv\n",
      "Deleted file: ./data\\fhv_tripdata_2019-11.csv.gz\n",
      "Deleted file: ./data\\fhv_tripdata_2019-12.csv\n",
      "Deleted file: ./data\\fhv_tripdata_2019-12.csv.gz\n",
      "All files processed, uploaded to BigQuery, and purged from the download directory.\n"
     ]
    }
   ],
   "source": [
    " # Purge files in the download directory\n",
    "purge_files(DOWNLOAD_DIR)\n",
    "\n",
    "print(\"All files processed, uploaded to BigQuery, and purged from the download directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DE2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
